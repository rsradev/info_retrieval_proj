{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can be used to implement sentiment analysis and text features like TF-IDF, n-grams, and word embeddings. Spark is well-suited for handling large-scale data processing tasks, making it and ideal platform for processing large datasets efficiently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dataset collection and load text and libs\n",
    "Dataset from Kaggle -> https://www.kaggle.com/datasets/emineyetm/fake-news-detection-datasets?resource=download (Fake news detection dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.38:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FakeNewsDetection</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x106ab7e50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    \n",
    "            SparkSession\n",
    "            .builder\n",
    "            .appName('FakeNewsDetection') \n",
    "            .config(\"spark.driver.memory\", \"4g\")\n",
    "            .config(\"spark.executor.memory\", \"8g\")\n",
    "            .master('local[*]') \n",
    "            .getOrCreate()\n",
    "    )\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StringType, \n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "    StructField('subject', StringType(), True),\n",
    "    StructField('date', StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [\n",
    "    \"politicsNews\",\n",
    "    \"worldnews\",\n",
    "    \"News\",\n",
    "    \"politics\",\n",
    "    \"left-news\",\n",
    "    \"US_News\",\n",
    "    \"Middle-east\",\n",
    "    \"Government News\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = spark.read.csv(\"./data/Fake.csv\", header=True, schema=schema)#.limit(5000)\n",
    "df_true = spark.read.csv(\"./data/True.csv\", header=True, schema=schema)#.limit(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenize the text, remove stopwords, and compute TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_true.withColumn('fake', F.lit(0)).union(df_fake.withColumn('fake', F.lit(1))).orderBy(F.rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter(F.col('subject').isin(subjects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43638"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the values of the subject column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer, RegexTokenizer, StopWordsRemover, CountVectorizer, Imputer, IDF\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "StopWordsRemover.loadDefaultStopWords('english')\n",
    "\n",
    "# 0. Extract tokens from title\n",
    "title_tokenizer= RegexTokenizer(inputCol= 'title', outputCol= 'title_words',\n",
    "                                pattern= '\\\\W', toLowercase= True)\n",
    "# 1. Remove stop words from title\n",
    "title_sw_remover= StopWordsRemover(inputCol= 'title_words', outputCol= 'title_sw_removed')\n",
    "# 2. Compute Term frequency from title\n",
    "title_count_vectorizer= CountVectorizer(inputCol= 'title_sw_removed', outputCol= 'tf_title')\n",
    "# 3. Compute Term frequency-inverse document frequency from title\n",
    "title_tfidf= IDF(inputCol= 'tf_title', outputCol= 'tf_idf_title')\n",
    "# 4. Extract tokens from text\n",
    "text_tokenizer= RegexTokenizer(inputCol= 'text', outputCol= 'text_words',\n",
    "                                pattern= '\\\\W', toLowercase= True)\n",
    "# 5. Remove stop words from text\n",
    "text_sw_remover= StopWordsRemover(inputCol= 'text_words', outputCol= 'text_sw_removed')\n",
    "# 6. Compute Term frequency from text\n",
    "text_count_vectorizer= CountVectorizer(inputCol= 'text_sw_removed', outputCol= 'tf_text')\n",
    "# 7. Compute Term frequency-inverse document frequency text\n",
    "text_tfidf= IDF(inputCol= 'tf_text', outputCol= 'tf_idf_text')\n",
    "# 8. StringIndexer subject\n",
    "subject_str_indexer= StringIndexer(inputCol= 'subject', outputCol= 'subject_idx')\n",
    "# 9. VectorAssembler\n",
    "vec_assembler= VectorAssembler(inputCols=['tf_idf_title', 'tf_idf_text', 'subject_idx'], outputCol= 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# 10 Random Forest Classifier\n",
    "rf= RandomForestClassifier(featuresCol= 'features', labelCol= 'fake', predictionCol= 'fake_predict', maxDepth= 7, numTrees= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "rf_pipe= Pipeline(stages=[title_tokenizer, # 0\n",
    "                title_sw_remover, # 1\n",
    "                title_count_vectorizer, # 2\n",
    "                title_tfidf, # 3\n",
    "                text_tokenizer, # 4\n",
    "                text_sw_remover, # 5\n",
    "                text_count_vectorizer, # 6\n",
    "                text_tfidf, # 7\n",
    "                subject_str_indexer, # 8\n",
    "                vec_assembler, # 9\n",
    "                rf]) # 10 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test= data.randomSplit([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/15 17:44:16 WARN DAGScheduler: Broadcasting large task binary with size 1207.6 KiB\n",
      "24/12/15 17:44:18 WARN DAGScheduler: Broadcasting large task binary with size 1208.6 KiB\n",
      "24/12/15 17:44:23 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n",
      "24/12/15 17:44:23 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n",
      "24/12/15 17:44:26 WARN DAGScheduler: Broadcasting large task binary with size 11.8 MiB\n",
      "24/12/15 17:44:40 WARN DAGScheduler: Broadcasting large task binary with size 13.4 MiB\n",
      "24/12/15 17:44:42 WARN MemoryStore: Not enough space to cache rdd_655_6 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:44:42 WARN BlockManager: Persisting block rdd_655_6 to disk instead.\n",
      "24/12/15 17:44:42 WARN MemoryStore: Not enough space to cache rdd_655_5 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:44:42 WARN BlockManager: Persisting block rdd_655_5 to disk instead.\n",
      "24/12/15 17:44:42 WARN MemoryStore: Not enough space to cache rdd_655_7 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:44:42 WARN BlockManager: Persisting block rdd_655_7 to disk instead.\n",
      "24/12/15 17:44:42 WARN MemoryStore: Not enough space to cache rdd_655_4 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:44:42 WARN BlockManager: Persisting block rdd_655_4 to disk instead.\n",
      "24/12/15 17:44:42 WARN MemoryStore: Not enough space to cache rdd_655_1 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:44:42 WARN BlockManager: Persisting block rdd_655_1 to disk instead.\n",
      "24/12/15 17:44:42 WARN MemoryStore: Not enough space to cache rdd_655_3 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:44:42 WARN BlockManager: Persisting block rdd_655_3 to disk instead.\n",
      "24/12/15 17:44:42 WARN MemoryStore: Not enough space to cache rdd_655_2 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:44:42 WARN BlockManager: Persisting block rdd_655_2 to disk instead.\n",
      "24/12/15 17:44:42 WARN MemoryStore: Not enough space to cache rdd_655_0 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:44:42 WARN BlockManager: Persisting block rdd_655_0 to disk instead.\n",
      "24/12/15 17:44:47 WARN MemoryStore: Not enough space to cache rdd_655_8 in memory! (computed 1048.4 MiB so far)\n",
      "24/12/15 17:44:47 WARN BlockManager: Persisting block rdd_655_8 to disk instead.\n",
      "24/12/15 17:44:47 WARN MemoryStore: Not enough space to cache rdd_655_9 in memory! (computed 1048.4 MiB so far)\n",
      "24/12/15 17:44:47 WARN BlockManager: Persisting block rdd_655_9 to disk instead.\n",
      "24/12/15 17:44:52 WARN MemoryStore: Not enough space to cache rdd_655_5 in memory! (computed 31.0 MiB so far)\n",
      "24/12/15 17:44:52 WARN MemoryStore: Not enough space to cache rdd_655_8 in memory! (computed 300.3 MiB so far)\n",
      "24/12/15 17:44:52 WARN MemoryStore: Not enough space to cache rdd_655_3 in memory! (computed 300.3 MiB so far)\n",
      "24/12/15 17:44:52 WARN MemoryStore: Not enough space to cache rdd_655_2 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:44:52 WARN MemoryStore: Not enough space to cache rdd_655_4 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:44:52 WARN MemoryStore: Not enough space to cache rdd_655_6 in memory! (computed 453.1 MiB so far)\n",
      "24/12/15 17:44:52 WARN MemoryStore: Not enough space to cache rdd_655_9 in memory! (computed 130.0 MiB so far)\n",
      "24/12/15 17:44:54 WARN MemoryStore: Not enough space to cache rdd_655_1 in memory! (computed 453.1 MiB so far)\n",
      "24/12/15 17:44:54 WARN MemoryStore: Not enough space to cache rdd_655_7 in memory! (computed 1048.4 MiB so far)\n",
      "24/12/15 17:44:57 WARN MemoryStore: Not enough space to cache rdd_655_0 in memory! (computed 1611.8 MiB so far)\n",
      "24/12/15 17:44:59 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "24/12/15 17:45:00 WARN MemoryStore: Not enough space to cache rdd_655_2 in memory! (computed 300.3 MiB so far)\n",
      "24/12/15 17:45:00 WARN MemoryStore: Not enough space to cache rdd_655_6 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:00 WARN MemoryStore: Not enough space to cache rdd_655_8 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:00 WARN MemoryStore: Not enough space to cache rdd_655_1 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:00 WARN MemoryStore: Not enough space to cache rdd_655_5 in memory! (computed 130.0 MiB so far)\n",
      "24/12/15 17:45:00 WARN MemoryStore: Not enough space to cache rdd_655_7 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:00 WARN MemoryStore: Not enough space to cache rdd_655_0 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:00 WARN MemoryStore: Not enough space to cache rdd_655_3 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:00 WARN MemoryStore: Not enough space to cache rdd_655_4 in memory! (computed 130.0 MiB so far)\n",
      "24/12/15 17:45:03 WARN MemoryStore: Not enough space to cache rdd_655_9 in memory! (computed 1611.8 MiB so far)\n",
      "24/12/15 17:45:05 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "24/12/15 17:45:06 WARN MemoryStore: Not enough space to cache rdd_655_0 in memory! (computed 130.0 MiB so far)\n",
      "24/12/15 17:45:06 WARN MemoryStore: Not enough space to cache rdd_655_1 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:06 WARN MemoryStore: Not enough space to cache rdd_655_8 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:06 WARN MemoryStore: Not enough space to cache rdd_655_6 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:06 WARN MemoryStore: Not enough space to cache rdd_655_7 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:06 WARN MemoryStore: Not enough space to cache rdd_655_2 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:06 WARN MemoryStore: Not enough space to cache rdd_655_3 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:07 WARN MemoryStore: Not enough space to cache rdd_655_4 in memory! (computed 453.1 MiB so far)\n",
      "24/12/15 17:45:08 WARN MemoryStore: Not enough space to cache rdd_655_9 in memory! (computed 683.9 MiB so far)\n",
      "24/12/15 17:45:10 WARN MemoryStore: Not enough space to cache rdd_655_5 in memory! (computed 1611.8 MiB so far)\n",
      "24/12/15 17:45:11 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "24/12/15 17:45:12 WARN MemoryStore: Not enough space to cache rdd_655_7 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:12 WARN MemoryStore: Not enough space to cache rdd_655_5 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:12 WARN MemoryStore: Not enough space to cache rdd_655_4 in memory! (computed 130.0 MiB so far)\n",
      "24/12/15 17:45:12 WARN MemoryStore: Not enough space to cache rdd_655_9 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:13 WARN MemoryStore: Not enough space to cache rdd_655_0 in memory! (computed 300.3 MiB so far)\n",
      "24/12/15 17:45:13 WARN MemoryStore: Not enough space to cache rdd_655_2 in memory! (computed 300.3 MiB so far)\n",
      "24/12/15 17:45:13 WARN MemoryStore: Not enough space to cache rdd_655_8 in memory! (computed 300.3 MiB so far)\n",
      "24/12/15 17:45:13 WARN MemoryStore: Not enough space to cache rdd_655_6 in memory! (computed 300.3 MiB so far)\n",
      "24/12/15 17:45:13 WARN MemoryStore: Not enough space to cache rdd_655_1 in memory! (computed 300.3 MiB so far)\n",
      "24/12/15 17:45:13 WARN MemoryStore: Not enough space to cache rdd_655_3 in memory! (computed 300.3 MiB so far)\n",
      "24/12/15 17:45:16 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "24/12/15 17:45:17 WARN MemoryStore: Not enough space to cache rdd_655_2 in memory! (computed 130.0 MiB so far)\n",
      "24/12/15 17:45:17 WARN MemoryStore: Not enough space to cache rdd_655_0 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:17 WARN MemoryStore: Not enough space to cache rdd_655_5 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:17 WARN MemoryStore: Not enough space to cache rdd_655_3 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:17 WARN MemoryStore: Not enough space to cache rdd_655_7 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:17 WARN MemoryStore: Not enough space to cache rdd_655_9 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:17 WARN MemoryStore: Not enough space to cache rdd_655_1 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:17 WARN MemoryStore: Not enough space to cache rdd_655_6 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:17 WARN MemoryStore: Not enough space to cache rdd_655_4 in memory! (computed 199.7 MiB so far)\n",
      "24/12/15 17:45:22 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "24/12/15 17:45:22 WARN MemoryStore: Not enough space to cache rdd_655_2 in memory! (computed 31.0 MiB so far)\n",
      "24/12/15 17:45:22 WARN MemoryStore: Not enough space to cache rdd_655_1 in memory! (computed 31.0 MiB so far)\n",
      "24/12/15 17:45:22 WARN MemoryStore: Not enough space to cache rdd_655_5 in memory! (computed 53.8 MiB so far)\n",
      "24/12/15 17:45:22 WARN MemoryStore: Not enough space to cache rdd_655_6 in memory! (computed 53.8 MiB so far)\n",
      "24/12/15 17:45:22 WARN MemoryStore: Not enough space to cache rdd_655_3 in memory! (computed 53.8 MiB so far)\n",
      "24/12/15 17:45:22 WARN MemoryStore: Not enough space to cache rdd_655_0 in memory! (computed 53.8 MiB so far)\n",
      "24/12/15 17:45:23 WARN MemoryStore: Not enough space to cache rdd_655_4 in memory! (computed 84.3 MiB so far)\n",
      "24/12/15 17:45:23 WARN MemoryStore: Not enough space to cache rdd_655_9 in memory! (computed 130.0 MiB so far)\n",
      "24/12/15 17:45:23 WARN MemoryStore: Not enough space to cache rdd_655_7 in memory! (computed 300.3 MiB so far)\n",
      "24/12/15 17:45:27 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "24/12/15 17:45:28 WARN MemoryStore: Not enough space to cache rdd_655_0 in memory! (computed 15.7 MiB so far)\n",
      "24/12/15 17:45:28 WARN MemoryStore: Not enough space to cache rdd_655_1 in memory! (computed 53.8 MiB so far)\n",
      "24/12/15 17:45:28 WARN MemoryStore: Not enough space to cache rdd_655_4 in memory! (computed 53.8 MiB so far)\n",
      "24/12/15 17:45:28 WARN MemoryStore: Not enough space to cache rdd_655_7 in memory! (computed 31.0 MiB so far)\n",
      "24/12/15 17:45:28 WARN MemoryStore: Not enough space to cache rdd_655_9 in memory! (computed 84.3 MiB so far)\n",
      "24/12/15 17:45:28 WARN MemoryStore: Not enough space to cache rdd_655_3 in memory! (computed 84.3 MiB so far)\n",
      "24/12/15 17:45:28 WARN MemoryStore: Not enough space to cache rdd_655_5 in memory! (computed 84.3 MiB so far)\n",
      "24/12/15 17:45:28 WARN MemoryStore: Not enough space to cache rdd_655_2 in memory! (computed 130.0 MiB so far)\n",
      "24/12/15 17:45:29 WARN MemoryStore: Not enough space to cache rdd_655_6 in memory! (computed 300.3 MiB so far)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rf_model= rf_pipe.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import  MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "accuracy= MulticlassClassificationEvaluator(labelCol= 'fake', predictionCol= 'fake_predict', metricName= 'accuracy')\n",
    "f1= MulticlassClassificationEvaluator(labelCol= 'fake', predictionCol= 'fake_predict', metricName= 'f1')\n",
    "areaUnderROC= BinaryClassificationEvaluator(labelCol= 'fake', metricName= 'areaUnderROC')\n",
    "\n",
    "def classification_evaluator(data_result):\n",
    "    data_result.crosstab(col1= 'fake_predict', col2= 'fake').show()\n",
    "    print('accuracy:' ,accuracy.evaluate(data_result))\n",
    "    print('f1:' ,f1.evaluate(data_result))\n",
    "    print('areaUnderROC:' ,areaUnderROC.evaluate(data_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training data set\n",
    "rf_train_result= rf_model.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/15 17:45:36 WARN DAGScheduler: Broadcasting large task binary with size 10.8 MiB\n",
      "24/12/15 17:45:38 WARN DAGScheduler: Broadcasting large task binary with size 10.7 MiB\n",
      "24/12/15 17:45:39 WARN DAGScheduler: Broadcasting large task binary with size 10.7 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----+\n",
      "|fake_predict_fake|    0|    1|\n",
      "+-----------------+-----+-----+\n",
      "|              1.0|  433|17119|\n",
      "|              0.0|16598|  627|\n",
      "+-----------------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/15 17:45:40 WARN DAGScheduler: Broadcasting large task binary with size 10.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9700106386037551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/15 17:45:44 WARN DAGScheduler: Broadcasting large task binary with size 10.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.9690793761914358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/15 17:45:47 WARN DAGScheduler: Broadcasting large task binary with size 10.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC: 0.9945007189369528\n"
     ]
    }
   ],
   "source": [
    "classification_evaluator(rf_train_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_result= rf_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/15 17:45:52 WARN DAGScheduler: Broadcasting large task binary with size 10.8 MiB\n",
      "24/12/15 17:45:54 WARN DAGScheduler: Broadcasting large task binary with size 10.7 MiB\n",
      "24/12/15 17:45:54 WARN DAGScheduler: Broadcasting large task binary with size 10.7 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+----+\n",
      "|fake_predict_fake|   0|   1|\n",
      "+-----------------+----+----+\n",
      "|              1.0|  97|4430|\n",
      "|              0.0|4177| 155|\n",
      "+-----------------+----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/15 17:45:55 WARN DAGScheduler: Broadcasting large task binary with size 10.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9668739400791407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/15 17:45:58 WARN DAGScheduler: Broadcasting large task binary with size 10.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.9683937540859505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/15 17:46:00 WARN DAGScheduler: Broadcasting large task binary with size 10.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC: 0.994842927030133\n"
     ]
    }
   ],
   "source": [
    "classification_evaluator(rf_test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info-retrieval-proj-yqIGXVWS-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
